{"cells":[{"cell_type":"markdown","metadata":{"id":"L4IEmrCq8Wxf"},"source":["# CS 505 Homework 05:  Recurrent Neural Networks\n","\n","#### Due Monday  11/27 at midnight (1 minute after 11:59 pm) in Gradescope (with a grace period of 6 hours)\n","#### You may submit the homework up to 24 hours late (with the same grace period) for a penalty of 10%.\n","\n","All homeworks will be scored with a maximum of 100 points; point values are given\n","for individual problems, and if parts of problems do not have point values given, they\n","will be counted equally toward the total for that problem.\n","\n","Note: This homework is a bit different from the first four in this class in that in some parts we are specified **what** you need to do for your solutions, but much less of the **how** you write the details of the code. There are three reasons for this:\n","\n","- In a graduate level CS class, after four homeworks and two months of lectures, you should be well-equipped to work out the coding issues for yourself, and in general, going forward, this is how you will solve the kinds of problems presented here;\n","- Suggestions for resources (mostly ML blogs) will be suggested; there are many resources, but these are from bloggers that I trust and have used in the past;\n","- I am expecting that you will make good use of chatGPT for help with the details of syntax and low-level organization of your code. There is often nothing very stimulating or informative about precisely what is the syntax needed for a particular kind of layer in a network, and rather than poke around on StackOverflow, chatGPT is particularly good at summarizing existing approaches to ML coding tasks.\n","\n","#### Submission Instructions\n","\n","You must complete the homework by editing <b>this notebook</b> and submitting the following two files in Gradescope by the due date and time:\n","\n","  - A file <code>HW05.ipynb</code> (be sure to select <code>Kernel -> Restart and Run All</code> before you submit, to make sure everything works); and\n","  - A file <code>HW05.pdf</code> created from the previous.\n","  \n","  For best results obtaining a clean PDF file on the Mac, select <code>File -> Print Review</code> from the Jupyter window, then choose <code>File-> Print</code> in your browser and then <code>Save as PDF</code>.  Something  similar should be possible on a Windows machine -- just make sure it is readable and no cell contents have been cut off. Make it easy to grade!\n","  \n","The date and time of your submission is the last file you submitted, so if your IPYNB file is submitted on time, but your PDF is late, then your submission is late."]},{"cell_type":"markdown","metadata":{"id":"vAsi-ORE8Wxf"},"source":["## Collaborators (5 pts)\n","\n","Describe briefly but precisely\n","\n","1. Any persons you discussed this homework with and the nature of the discussion;\n","2. Any online resources you consulted and what information you got from those resources; and\n","3. Any AI agents (such as chatGPT or CoPilot) or other applications you used to complete the homework, and the nature of the help you received.\n","\n","A few brief sentences is all that I am looking for here.\n","\n","I collaborated with Phillip Tran. I occasionally used chatGPT for minor debugging."]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":15636,"status":"ok","timestamp":1700521000822,"user":{"displayName":"Alex Lavaee","userId":"12456354411202670063"},"user_tz":300},"id":"zSdvpPwM8Wxh","scrolled":false},"outputs":[],"source":["import math\n","import numpy as np\n","from numpy.random import shuffle, seed, choice\n","from tqdm import tqdm\n","from collections import defaultdict, Counter\n","import pandas as pd\n","import re\n","import matplotlib.pyplot as plt\n","\n","import torch\n","from torch.utils.data import Dataset,DataLoader\n","import torch.nn.functional as F\n","from torch.utils.data import random_split,Dataset,DataLoader\n","from torchvision import datasets, transforms\n","from torch import nn, optim\n","\n","import torchvision.transforms as T\n","\n","from sklearn.decomposition import PCA, TruncatedSVD\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"]},{"cell_type":"markdown","metadata":{"id":"yZmCUzJb8Wxh"},"source":["## Problem One:  Character-Level Generative Model (20 pts)\n","\n","A basic character-level model has been provided on the class web site in the row for Lecture 14:\n","<a href=\"https://www.cs.bu.edu/fac/snyder/cs505/CharacterLevelLSTM.ipynb\">IPYNB</a>. Your first step is to download this and run it in Colab (or download the data file, which is in the CS 505 Data Directory and also linked on the web site, and run it on your local machine) and understand all its various features. Most of it is straight-forward at this point in the course, but the definition of the model is a bit messy, and you will need to read about LSTM layers in the Pytorch documents to really understand what it is doing and what the hyperparameters mean.\n","\n","Also take a look at the article \"The Unreasonable Effectiveness of Recurrent Neural Networks\" linked with lecture 14.\n","\n","For this problem, you will run this code on a dataset consisting of Java code files, which has been uploaded to the CS 505 Data Directory and also to the class web site: <a href=\"https://www.cs.bu.edu/fac/snyder/cs505/JavaFiles/\">DIR</a>  Select some number of these files and concatenate them into one long text file, such that you have approximately 10-20K characters (if you have trouble running out of RAM you can use fewer, but try to get at least 10K).\n","\n","You will run the character-level model on this dataset. You may either cut and paste code into this notebook, or submit the file with your changes and output along with this notebook to Gradescope.\n","\n","Your task is to get a character-level model that has not simply memorized the Java text file by overfitting, and does not do much other than spit out random characters (underfitting).  You will get the former if you simply run it for many epochs without any changes to the hyperparameters; you will get the latter if you run it only a few epochs.\n","\n","You should experiment with different hyperparameters, which in the notebook are indicated\n","by\n","\n","          <== something to play with\n","\n","and try to get a model that seems to recognize typical Java syntax such as comments, matching parentheses, expressions, assignments, and formatting, but is not just repeating\n","exact text from the data file. Clearly, the number of epochs plays a crucial role, but I also want you to\n","experiment with the various hyperparameters to try to avoid overfitting. See my lectures on T 10/31 and Th 11/2 (recorded and on my YT channel) for the background to this.\n","\n","Note that the code you will work from does not use validation and testing sets, nor does it calculate the accuracy, but only tracks the loss. The nature of the data sets for character-level models does not seem to lend itself to accuracy metrics, but you may wish to try this -- I have not found it to be useful, but have simply focussed on the output and \"eyeballed\" the results to determine how much they have generalized\n","from the data.\n","\n","Submit your notebook(s) to Gradescope as usual, and also provide a summary of your results in the next cell.\n"]},{"cell_type":"markdown","metadata":{"id":"aibUCNT2-FTi"},"source":["## Problem One Solution"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"o62hMx5Z97tq"},"outputs":[],"source":["from pathlib import Path\n","from typing import List, Tuple, Any, Dict, Optional\n","import numpy as np\n","\n","DATA_DIR = Path(\"data\")\n","MAX_CHARS = 20000"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["java_corpus_path = DATA_DIR.joinpath(\"java\", \"java_corpus.txt\")\n","with open(java_corpus_path) as f:\n","    corpus = f.read()[:MAX_CHARS]"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def create_sequences(corpus: str, sequence_len: int = 128) -> Tuple[List[str], List[str]]:\n","    input_sequences = []\n","    target_sequences = []\n","    \n","    for i in range(len(corpus) - sequence_len - 1):\n","        input_sequences.append(corpus[i: i + sequence_len])\n","        target_sequences.append(corpus[i + 1: i + 1 + sequence_len])\n","        \n","    return input_sequences, target_sequences"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Input sequence:\n"," /* File: BSTExperiment.java\n"," * Authors: Brian Borucki and Wayne Snyder\n"," * Date: 9/10/13\n"," * Purpose: This is code for a lecture d\n","Target sequence:\n"," * File: BSTExperiment.java\n"," * Authors: Brian Borucki and Wayne Snyder\n"," * Date: 9/10/13\n"," * Purpose: This is code for a lecture de\n"]}],"source":["input_sequences, target_sequences = create_sequences(corpus, 128)\n","\n","print(\"Input sequence:\\n\", input_sequences[0])\n","print(\"Target sequence:\\n\", target_sequences[0])"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["chars = sorted(list(set(corpus)))\n","embedding_table = dict(zip(chars, range(len(chars))))\n","reverse_embedding_table = {value: key for key, value in embedding_table.items()}"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def encode_sequence(sequence: List[str], \n","                    embedding_table: Dict[str, int]) -> np.ndarray:\n","    embedding_dim = len(embedding_table)\n","    embedding_matrix = np.eye(embedding_dim)\n","    \n","    sequence_ids = [embedding_table[s] for s in sequence]\n","    \n","    return embedding_matrix[sequence_ids]"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def decode_sequence(embedding: np.ndarray, \n","                    reverse_embedding_table: Dict[str, int]) -> str:\n","    \n","    sequence_ids = np.argmax(embedding, axis=1)\n","    \n","    text = [reverse_embedding_table[s] for s in sequence_ids]\n","    \n","    return \"\".join(text)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["embedding = encode_sequence(sequence=input_sequences[0], embedding_table=embedding_table)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/* File: BSTExperiment.java\n"," * Authors: Brian Borucki and Wayne Snyder\n"," * Date: 9/10/13\n"," * Purpose: This is code for a lecture d\n","/* File: BSTExperiment.java\n"," * Authors: Brian Borucki and Wayne Snyder\n"," * Date: 9/10/13\n"," * Purpose: This is code for a lecture d\n"]}],"source":["print(input_sequences[0])\n","print(decode_sequence(embedding=embedding,\n","                      reverse_embedding_table=reverse_embedding_table))"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["class CharacterDataset(Dataset):\n","    def __init__(self, corpus: str, sequence_len: int = 128) -> None:\n","        self.corpus = corpus\n","        self.chars = sorted(list(set(self.corpus)))\n","        self.embedding_table = dict(zip(self.chars, range(len(self.chars))))\n","        self.reverse_embedding_table = {value: key for key, value \n","                                        in self.embedding_table.items()}\n","        self.sequence_len = sequence_len\n","        self.input_sequences, self.target_sequences = create_sequences(self.corpus, \n","                                                                       sequence_len=self.sequence_len)\n","        \n","        self.input_sequences = torch.stack([torch.tensor(encode_sequence(sequence=input_sequence, \n","                                                            embedding_table=embedding_table),\n","                                                         dtype=torch.float32)\n","                                            for input_sequence in input_sequences], dim=0)\n","        \n","        self.target_sequences = torch.stack([torch.tensor(encode_sequence(sequence=target_sequence,\n","                                                                          embedding_table=embedding_table),\n","                                                         dtype=torch.float32)\n","                                            for target_sequence in target_sequences], dim=0)\n","        \n","    def __len__(self) -> int:\n","        return self.input_sequences.size(0)\n","    \n","    def __getitem__(self, index) -> Any:\n","        return (self.input_sequences[index], \n","                self.target_sequences[index])"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["BS = 128\n","dataset = CharacterDataset(corpus=corpus, sequence_len=128)\n","dataloader = DataLoader(dataset=dataset, batch_size=BS)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU is being used.\n"]}],"source":["# set device\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print(\"GPU is being used.\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"CPU is being used.\")"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["import torch.nn as nn\n","\n","class Model(nn.Module):\n","    def __init__(self, input_size: int) -> None:\n","        super().__init__()\n","        \n","        self.lstm = nn.LSTM(input_size=input_size,\n","                                  hidden_size=128,\n","                                  num_layers=2,\n","                                  batch_first=True,\n","                                  dropout=0.0)\n","        self.fcn = nn.Linear(in_features=128, out_features=input_size)\n","        \n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        output, (h_n, c_n) = self.lstm(x)\n","        output = self.fcn(output)\n","        return output"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["input_size = len(dataset.chars)\n","model = Model(input_size=input_size).to(device)"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([4, 128, 80])"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["dataset[:4][0].size()"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([4, 128, 80])"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["model(dataset[:4][0].to(device)).shape"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["from tqdm import tqdm\n","\n","def train(num_epochs: int,\n","          early_stopping: bool = True,\n","          save_model_dir: Optional[str] = None,\n","          patience: int = 3) -> Dict[str, List[float]]:\n","\n","  best_epoch_idx = 0\n","  p = 0\n","\n","  if save_model_dir:\n","    save_model_dir = Path(save_model_dir)\n","    if not save_model_dir.exists():\n","      save_model_dir.mkdir()\n","\n","  history = dict()\n","  history[\"loss\"] = []\n","\n","  for epoch in range(num_epochs):\n","\n","    with tqdm(total=len(dataloader), unit=\"batch\",\n","              desc=f\"Epoch {epoch+1}\") as pbar:\n","\n","      N = 0\n","      running_loss = 0.0\n","\n","      for X, y in dataloader:\n","        X, y = X.cuda(), y.cuda()\n","\n","        optimizer.zero_grad()\n","\n","        y_pred = model(X)\n","\n","        loss = loss_fn(y_pred, y)\n","        running_loss += (loss.cpu().item() * len(y))\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        y_pred = torch.argmax(F.softmax(y_pred, dim=-1))\n","\n","        N += len(y)\n","\n","        pbar.update(1)\n","\n","        pbar.set_postfix(loss=loss.item())\n","\n","      history[\"loss\"].append(running_loss / N)\n","\n","      pbar.close()\n","\n","      if early_stopping:\n","        if p == patience:\n","          break\n","\n","        if loss < history[\"loss\"][best_epoch_idx]:\n","          p = 0\n","          best_epoch_idx = epoch\n","          torch.save(model.state_dict(),\n","                     save_model_dir.joinpath(\n","                        f\"epoch_{best_epoch_idx}_val_loss_{loss}.pth\"))\n","        else:\n","          p += 1\n","\n","  return history"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch 1: 100%|██████████| 156/156 [00:01<00:00, 85.17batch/s, loss=6.42]\n","Epoch 2: 100%|██████████| 156/156 [00:01<00:00, 88.12batch/s, loss=6.26]\n","Epoch 3: 100%|██████████| 156/156 [00:01<00:00, 88.84batch/s, loss=6.12]\n","Epoch 4: 100%|██████████| 156/156 [00:01<00:00, 88.30batch/s, loss=6.01]\n"]}],"source":["history = train(num_epochs=100,\n","                early_stopping=True,\n","                save_model_dir=\"saved_models\",\n","                patience=3)"]},{"cell_type":"markdown","metadata":{"id":"VnB5z-ZoZEa5"},"source":["### Your analysis\n","\n","Please describe your experiments and cut and paste various outputs to show how the model performed at\n","various numbers of epochs and with various hyperparameters. What characteristics of Java was it able to learn? What did it not learn? The article \"The Unreasonable ...\" does a nice job of showing this kind of behavior as the number of epochs increases, and you might look at it before writing your answer here.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"AP4VdFkR8Wxi"},"source":["## Problem Two:  Word-Level Generative Model (40 pts)\n","\n","In this problem you will write another generative model, as you did in HW 03, but this time you will use an LSTM network, GloVe word embeddings, and beam search.\n","\n","Before you start, read the following blog post to see the core ideas involved in creating a generative model using word embeddings:\n","\n","https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/\n","\n","You may also wish to consult with chatGPT about how to develop this kind of model in Pytorch.\n","\n","The requirements for this problem are as follows (they mostly consist of the extensions proposed at\n","the end of the blog post linked above):\n","\n","- Develop your code in Pytorch, not Keras\n","- Use the novel *Persuation* by Jane Austen as your training data (available through the NLTK, you can just grab the sentences using `nltk.corpus.gutenberg.sents('austen-persuasion.txt')`); if you have trouble with RAM you will need to cut down the number of sentences (perhaps by eliminating the longest sentences as well, see next point).\n","- Develop a sentence-level model by padding sentences to the maximum sentence length in the novel (if this seems extreme, you may wish to delete a small number of the longest sentences to reduce the maximum length). Surround your data sentences with `<s>` and `</s>` and your model should generate one sentence at a time (as you did in HW 03), i.e., it should stop if it generates the `</s>` token.\n","- Use pretrained GLoVe embeddings with dimension 200, and update them (refine by training further) on the sentences in the novel; if you have trouble with RAM you may use a smaller dimension.\n","- Experiment with the hyperparameters (sample length, number of layers, uni- or bi-directional, weight_decay, dropout, number of epochs, temperature of the softmax, etc.) as you did in Problem One to find the \"sweet spot\" where you are generating interesting-looking sentences but not simply repeating sentences from the data. You may want to try adding more linear layers on top to pick the most likely next word.\n","- Generate sentences using Beam Search, which we describe below.\n","\n","Your solution should be the code, samples of sentences generated with their score (described below), and your description of the investigation of various hyperparameters, and what strategy ended up seeming to generate the most realistic sentences that were not simply a repeat of sentences in the data.\n"]},{"cell_type":"markdown","metadata":{"id":"c3zQCQyy8Wxi"},"source":["### Beam Search\n","\n","Beam search was described, and example shown, in Lecture 14. Here is a brief pseudo-code explaination of what\n","you need to do:\n","\n","1. Develop your code as described above so that it can generate single sentences;\n","2. Copy enough of your code over from HW 03 so that you can calculate the perplexity of\n","        sentences (using the entire novel, or perhaps even a number of Jane Austen's novels as\n","        the data source). As an alternative, you may wish to do this separately, store the nested dictionary\n","        using Pickle, and load it here.\n","3. Calculate the probability distribution of sentences in your data source that you used in the previous step, similar to what you did at the end of HW 01.\n","4. Create a \"goodness function\" which estimates the quality of a sentence as the perplexity times the probability of its length.  This will be applied to all sequences of words, and not just sentences, but as a first approximation this is a way to attempt to make the distribution of sentence lengths similar to that in the novel.\n","5. Follow the description in slide 7 of Lecture 14 to generate until you have 10 finished sentences. Print these out with their perplexity, probability of their length, and the combined goodness metric."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EXNWYY3O8Wxj"},"outputs":[],"source":["# Code here"]},{"cell_type":"markdown","metadata":{"id":"R8vWaRuJ8Wxj"},"source":["### Analysis\n","\n","Describe what experiments you did with various alternatives as described above, and cut and paste examples illustrating your results."]},{"cell_type":"markdown","metadata":{"id":"7pC8Nqf08Wxj"},"source":["## Problem Three:  Part-of-Speech Tagging (40 pts)\n","\n","In this problem, we will experiment with three different approaches to the POS tagging problem, using\n","the Brown Corpus as our data set.\n","\n","Before starting this problem, please review Lecture 13 and download the file <a href=\"https://www.cs.bu.edu/fac/snyder/cs505/Viterbi_Algorithm.ipynb\">Viterbi_Algorithm.ipynb</a> from the\n","class web site.\n","\n","There are four parts to this problem:\n","\n","- Part A: You will establish a baseline accuracy for the task.\n","- Part B: Using the implementation of the Viterbi algorithm for Hidden Markov Models you downloaded, you will determine how much better than the baseline you can do with this very standard method.\n","- Part C: You will repeat the exercise of Part B, but using an LSTM implementation, exploring several options for the implementation of the LSTM layer.\n","- Part D: You will evaluate your results, comparing the various methods in the context of the baseline method from Part A.\n","- Optional: You may wish to try the same task with a transformer such as Bert.\n"]},{"cell_type":"markdown","metadata":{"id":"Hy9Lsskp8Wxk"},"source":["Recall that the Brown Corpus has a list of all sentences tagged with parts of speech. The tags are\n","a bit odd, and not generally used any more, so we will use a much simpler set of tags the `universal_tagset`.\n","\n","If you run the following cells, you will see that there are 57,340 sentences, tagged with 12 different tags.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IiZDRrRn8Wxk","outputId":"b39bd719-c8c8-4504-f455-60180afc3fbb","scrolled":false},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package brown to\n","[nltk_data]     /Users/waynesnyder/nltk_data...\n","[nltk_data]   Package brown is already up-to-date!\n","[nltk_data] Downloading package universal_tagset to\n","[nltk_data]     /Users/waynesnyder/nltk_data...\n","[nltk_data]   Package universal_tagset is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["There are 57340 sentences tagged with universal POS tags in the Brown Corpus.\n","\n","Here is the first sentence with universal tags: [('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')]\n"]}],"source":["import numpy as np\n","import nltk\n","\n","# The first time you will need to download the corpus:\n","\n","from nltk.corpus import brown\n","\n","nltk.download('brown')\n","nltk.download('universal_tagset')\n","\n","tagged_sentences = brown.tagged_sents(tagset='universal')\n","\n","print(f'There are {len(tagged_sentences)} sentences tagged with universal POS tags in the Brown Corpus.')\n","print(\"\\nHere is the first sentence with universal tags:\",tagged_sentences[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lzVJ74A_8Wxk","outputId":"802ce055-f4cf-4ecd-8849-8aa0de876b67","scrolled":false},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 12 universal tags in the Brown Corpus.\n","['.', 'ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRON', 'PRT', 'VERB', 'X']\n","\n"]}],"source":["# Uncomment to see the complete list of tags.\n","\n","all_tagged_words = np.concatenate(tagged_sentences)\n","all_tags = sorted(set([pos for (w,pos) in all_tagged_words]))\n","print(f'There are {len(all_tags)} universal tags in the Brown Corpus.')\n","print(all_tags)\n","print()\n"]},{"cell_type":"markdown","metadata":{"id":"qhDV9fCZ8Wxl"},"source":["### Part A\n","\n","In this part, you will establish a baseline for the task, using the naive method suggested on slide 35 of Lecture 13:\n","\n","- Tag every word with its most frequent POS tag (for example, if 'recent' is most frequently tagged as 'ADJ', then assume that every time 'recent' appears in a sentence, it should be tagged with 'ADJ');\n","- If a word has two or more most frequent tags, choose the one that appears first in the list of sorted tags above.\n","\n","Note that there will not be any \"unknown words.\"\n","\n","Use this method to determine your baseline accuracy (it may not be 92% as reported on slide 35!):\n","\n","- Build a dictionary mapping every word to its most frequent tag;\n","- Go through the entire tagged corpus, and report the accuracy (percentage of correct tags) of this baseline method.\n","\n","Do not tokenize or lower-case the words. Use the words and tags exactly as they are in the tagged sentences."]},{"cell_type":"markdown","metadata":{"id":"vvdJOskX8Wxl"},"source":["### Part B:  \n","\n","Now, review the `Viterbi.ipynb` notebook and read through Section 8.4 in Jurafsky & Martin to understand the basic approach that is used in the \"Janet will back the bill\" example. In detail:\n","\n","- Cut and paste the code from the Viterby notebook below and run your experiments in this notebook.\n","- You need to calculate from the Brown Corpus tagged sentences the probabilities for the various matrices used as input to the method:\n","   - `start_p`: This is the probability that a sentence starts with a given POS (in Figure 8.12 in J & M, this is given as the first line, in the row for `<s>`; simply collect the statistics for the first word in each sentence; it will be of size 1 x 12.\n","   - `trans_p`: This is the matrix of probabilities that one POS follows another in a sentence; build a 12 x 12 matrix of frequencies for whether the column POS follows the row POS in a sentence and then normalize each row so that it is a probability distribution (each row should add to 1.0)\n","   - `emit_p`: This is a matrix of size 12 x N, where N is the number of unique words in the corpus, which for each POS (the row) gives the probability that this POS in the output sequence corresponds to a specific word (the column) in the input sequence; again, you should collect frequency statistics about the relationship between POS and words, and normalize so that every row sums to 1.0.\n","   \n","Then run the algorithm on all the sentences in the tagged corpus, and determine the accuracy of the Viterbi algorithm. Again, the accuracy is calculated on each word, not on sentences as a whole.\n","\n","Report your results as a raw accuracy score, and in the two ways that were suggested on slide 12 of Lecture 11: percentage above the baseline established in Part A, and Cohen's Kappa."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dQIxLVIM8Wxl"},"outputs":[],"source":["# Viterbi code should be pasted here"]},{"cell_type":"markdown","metadata":{"id":"bptmi4vw8Wxm"},"source":["### Part C:  \n","\n","Next, you will need to develop an LSTM model to solve this problem. You may find it useful to\n","refer to the following, which presents an approach in Keras.\n","\n","https://www.kaggle.com/code/tanyadayanand/pos-tagging-using-rnn/notebook\n","\n","\n","You must do the following for this part:\n","\n","- Develop your code in Pytorch (of course!);\n","- Use pretrained GloVe embeddings of dimension 200 and update them with the brown sentences; if you run into problems with RAM, you may use a smaller embedding dimension;\n","- Truncate all sentences to a maximum of length 100 tokens, and pad shorter sentences (as in the reference above);\n","- Use an LSTM model and try several different choices for the parameters to the layer:\n","  - `hidden_size`:  Try several different widths for the layer\n","  - `bidirectional`: Try unidirectional (False) and bidirectional (True)\n","  - `num_layers`: Try 1 layer and 2 layers\n","  - `dropout`: In the case of 2 layers, try several different dropouts, including 0.\n","- Use early stopping with `patience = 50`;  \n","You do not have to try every possible combination of these parameter choices; a good strategy is to\n","try them separately, and then try a couple of combinations of the best choices of each.\n","\n","It is your choice about the other hyperparameters.  \n","\n","Provide a brief discussion of what you discovered, your best loss and accuracy measures for\n","validation, and three versions of your testing accuracy, as in Part B.  "]},{"cell_type":"markdown","metadata":{"id":"hc8hlBB98Wxm"},"source":["### Part D:  "]},{"cell_type":"markdown","metadata":{"id":"rk1GMHZr8Wxm"},"source":["Provide an analysis of what experiments you conducted with hyperparameters, what your results were, and in particular comment on how the two methods compare, especially given that one has *no* choice of hyperparameters, and one has *many* choices of parameters. How useful was the flexibility of choice in hyperparameters in Part C?"]},{"cell_type":"markdown","metadata":{"id":"jbrX23ja8Wxn"},"source":["#### Optional:\n","\n","You might want to try doing this problem with a transformer model such as BERT. There are plenty of blog posts out there describing the details, and, as usual, chatGPT would have plenty of things to say about the topic...."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}
